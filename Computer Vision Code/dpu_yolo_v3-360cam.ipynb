{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPU example: Yolo_v3\n",
    "\n",
    "This notebooks shows how to run a YOLO network based application for object detection. The application, as well as the DPU IP, is pulled from the official [Vitis AI Github Repository](https://github.com/Xilinx/Vitis-AI).\n",
    "For more information, please refer to the [Xilinx Vitis AI page](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html).\n",
    "\n",
    "In this notebook we will be using the DNNDK **Python API** to run the DPU tasks.\n",
    "\n",
    "## 1. Prepare the overlay\n",
    "We will download the overlay onto the board. Then we will load the \n",
    "corresponding DPU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq_dpu import DpuOverlay\n",
    "overlay = DpuOverlay(\"dpu.bit\")\n",
    "overlay.load_model(\"dpu_tf_yolov3.elf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants and helper functions \n",
    "\n",
    "You can view all of the helper functions in [DNNDK yolo example](https://github.com/Xilinx/Vitis-AI/blob/v1.1/mpsoc/vitis_ai_dnndk_samples/tf_yolov3_voc_py/tf_yolov3_voc.py). \n",
    "The helper functions released along with Vitis AI cover pre-processing of \n",
    "the images, so they can be normalized and resized to be compatible with \n",
    "the DPU model. These functions are included in our `pynq_dpu` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import colorsys\n",
    "from PIL import Image\n",
    "import pylab as plt\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "from pynq_dpu.edge.dnndk.tf_yolov3_voc_py.tf_yolov3_voc import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "Yolo V2 and V3 predict offsets from a predetermined set of boxes with \n",
    "particular height-width ratios; those predetermined set of boxes are the \n",
    "anchor boxes. We will use the predefined [anchors](https://github.com/Xilinx/Vitis-AI/blob/v1.1/mpsoc/vitis_ai_dnndk_samples/tf_yolov3_voc_py/model_data/yolo_anchors.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_list = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n",
    "anchor_float = [float(x) for x in anchor_list]\n",
    "anchors = np.array(anchor_float).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `get_class()` function in `tf_yolov3_voc` module to\n",
    "get class names from predefined [class names](https://github.com/Xilinx/Vitis-AI/blob/v1.1/mpsoc/vitis_ai_dnndk_samples/tf_yolov3_voc_py/image/voc_classes.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_path = \"voc_classes.txt\"\n",
    "class_names = get_class(classes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of classes, we will define a unique color for each\n",
    "class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "colors = list(map(lambda x: \n",
    "                  (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), \n",
    "                  colors))\n",
    "random.seed(0)\n",
    "random.shuffle(colors)\n",
    "random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some DPU-related parameters, such as DPU kernel name and\n",
    "input/output node names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_CONV=\"tf_yolov3\"\n",
    "CONV_INPUT_NODE=\"conv2d_1_convolution\"\n",
    "CONV_OUTPUT_NODE1=\"conv2d_59_convolution\"\n",
    "CONV_OUTPUT_NODE2=\"conv2d_67_convolution\"\n",
    "CONV_OUTPUT_NODE3=\"conv2d_75_convolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing bounding boxes\n",
    "We now define a custom function that draws the bounding boxes around \n",
    "the identified objects after we have the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, scores, classes):\n",
    "    image_h, image_w, _ = image.shape\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    fontScale = 3\n",
    "    thickness = 10\n",
    "    for i, bbox in enumerate(boxes):\n",
    "        [top, left, bottom, right] = bbox\n",
    "        width, height = right - left, bottom - top\n",
    "        center_x, center_y = left + width*0.5, top + height*0.5\n",
    "        score, class_index = scores[i], classes[i]\n",
    "        if(score > .6 and (class_names[class_index]==\"person\" or \n",
    "                           class_names[class_index]==\"car\" or \n",
    "                           class_names[class_index]==\"bicycle\" or\n",
    "                           class_names[class_index]==\"bus\"or\n",
    "                           class_names[class_index]==\"dog\"or\n",
    "                           class_names[class_index]==\"motorbike\"or\n",
    "                           class_names[class_index]==\"cat\"\n",
    "                          )):\n",
    "            label = '{}: {:.4f}'.format(class_names[class_index], score) \n",
    "\n",
    "            color = (0,255,0)\n",
    "\n",
    "            cv2.rectangle(image, (left,top), (right,bottom), color, thickness)\n",
    "            cv2.putText(image, label, (int(left), int(top-5)) , font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting classes\n",
    "We need to define a function that evaluates the scores and makes predictions\n",
    "based on the provided class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(yolo_outputs, image_shape, class_names, anchors):\n",
    "    score_thresh = 0.2\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    input_shape = np.shape(yolo_outputs[0])[1 : 3]\n",
    "    input_shape = np.array(input_shape)*32\n",
    "\n",
    "    for i in range(len(yolo_outputs)):\n",
    "        _boxes, _box_scores = boxes_and_scores(\n",
    "            yolo_outputs[i], anchors[anchor_mask[i]], len(class_names), \n",
    "            input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = np.concatenate(boxes, axis = 0)\n",
    "    box_scores = np.concatenate(box_scores, axis = 0)\n",
    "\n",
    "    mask = box_scores >= score_thresh\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(len(class_names)):\n",
    "        class_boxes_np = boxes[mask[:, c]]\n",
    "        class_box_scores_np = box_scores[:, c]\n",
    "        class_box_scores_np = class_box_scores_np[mask[:, c]]\n",
    "        nms_index_np = nms_boxes(class_boxes_np, class_box_scores_np) \n",
    "        class_boxes_np = class_boxes_np[nms_index_np]\n",
    "        class_box_scores_np = class_box_scores_np[nms_index_np]\n",
    "        classes_np = np.ones_like(class_box_scores_np, dtype = np.int32) * c\n",
    "        boxes_.append(class_boxes_np)\n",
    "        scores_.append(class_box_scores_np)\n",
    "        classes_.append(classes_np)\n",
    "    boxes_ = np.concatenate(boxes_, axis = 0)\n",
    "    scores_ = np.concatenate(scores_, axis = 0)\n",
    "    classes_ = np.concatenate(classes_, axis = 0)\n",
    "\n",
    "    return boxes_, scores_, classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run application\n",
    "\n",
    "We create DPU kernel and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2cube.dpuOpen()\n",
    "kernel = n2cube.dpuLoadKernel(KERNEL_CONV)\n",
    "task = n2cube.dpuCreateTask(kernel, 0)\n",
    "input_len = n2cube.dpuGetInputTensorSize(task, CONV_INPUT_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the DPU task to classify an input video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................ok\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import requests \n",
    "import json\n",
    "\n",
    "frame_width = int(cap.get(3)) \n",
    "frame_height = int(cap.get(4)) \n",
    "   \n",
    "size = (frame_width, frame_height) \n",
    "\n",
    "result = cv2.VideoWriter('myvideo.avi',  \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'), \n",
    "                         30, size) \n",
    "while(1):\n",
    "    try:\n",
    "        # Ricoh Theta API Take, Read and Delete Image\n",
    "        API_ENDPOINT = \"http://192.168.1.1:80/osc/commands/execute\"\n",
    "        data = {\"name\":\"camera.takePicture\"} \n",
    "        headers={\"Content-Type\":\"application/json\"}\n",
    "\n",
    "        # sending post request and saving response as response object \n",
    "        r = requests.post(url = API_ENDPOINT, data = json.dumps(data),headers=headers) \n",
    "\n",
    "        # extracting response text \n",
    "        pastebin_url = r.text \n",
    "\n",
    "        print(\"Take Picture Complete\") \n",
    "\n",
    "        idp=json.loads(r.text)[\"id\"]\n",
    "\n",
    "        print(\"Checking Url\") \n",
    "\n",
    "        API_ENDPOINT = \"http://192.168.1.1:80/osc/commands/status\"\n",
    "\n",
    "        data = {\"id\":idp} \n",
    "\n",
    "        headers={\"Content-Type\":\"application/json\"}\n",
    "\n",
    "        # sending post request and saving response as response object \n",
    "        r = requests.post(url = API_ENDPOINT, data = json.dumps(data),headers=headers) \n",
    "\n",
    "        # extracting response text \n",
    "        pastebin_url = json.loads(r.text) \n",
    "\n",
    "        while (json.loads(r.text)[\"state\"]!= \"done\"):\n",
    "            r = requests.post(url = API_ENDPOINT, data = json.dumps(data),headers=headers)\n",
    "            pastebin_url = json.loads(r.text)\n",
    "            print(\".\", end = '')\n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"Check Url Complete\") \n",
    "\n",
    "        img_url = pastebin_url[\"results\"][\"fileUrl\"]\n",
    "\n",
    "        with open('image.png', 'wb') as output_file,\\\n",
    "            requests.get(img_url, stream=True) as response:\n",
    "            shutil.copyfileobj(response.raw, output_file)\n",
    "\n",
    "\n",
    "        print(\"Save Image Complete Complete\") \n",
    "\n",
    "        API_ENDPOINT = \"http://192.168.1.1:80/osc/commands/execute\"\n",
    "\n",
    "        # your API key here \n",
    "        data = { \n",
    "            \"name\": \"camera.delete\", \n",
    "            \"parameters\": \n",
    "            {\n",
    "                \"fileUrls\":\n",
    "                [img_url]\n",
    "\n",
    "            }\n",
    "        }\n",
    "\n",
    "        headers={\"Content-Type\":\"application/json\"}\n",
    "\n",
    "        # sending post request and saving response as response object \n",
    "        r = requests.post(url = API_ENDPOINT, data = json.dumps(data),headers=headers) \n",
    "\n",
    "        # extracting response text \n",
    "        pastebin_url = r.text \n",
    "\n",
    "        print(\"Delete Image From Internal Storage Complete\") \n",
    "        \n",
    "        frame = cv.imread(\"image.png\") \n",
    "\n",
    "        # Start Time to Check FPS\n",
    "        start_time = time.time()\n",
    "        \n",
    "        image = frame\n",
    "        image_size = image.shape[:2]\n",
    "        image_data = np.array(pre_process(image, (416, 416)), dtype=np.float32)\n",
    "\n",
    "        n2cube.dpuSetInputTensorInHWCFP32(\n",
    "            task, CONV_INPUT_NODE, image_data, input_len)\n",
    "\n",
    "        n2cube.dpuRunTask(task)\n",
    "\n",
    "        conv_sbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE1)\n",
    "        conv_out1 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE1, \n",
    "                                                       conv_sbbox_size)\n",
    "        conv_out1 = np.reshape(conv_out1, (1, 13, 13, 75))\n",
    "\n",
    "        conv_mbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE2)\n",
    "        conv_out2 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE2, \n",
    "                                                       conv_mbbox_size)\n",
    "        conv_out2 = np.reshape(conv_out2, (1, 26, 26, 75))\n",
    "\n",
    "        conv_lbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE3)\n",
    "        conv_out3 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE3, \n",
    "                                                       conv_lbbox_size)\n",
    "        conv_out3 = np.reshape(conv_out3, (1, 52, 52, 75))\n",
    "\n",
    "        yolo_outputs = [conv_out1, conv_out2, conv_out3] \n",
    "\n",
    "        boxes, scores, classes = evaluate(yolo_outputs, image_size, \n",
    "                                      class_names, anchors)\n",
    "        print(\"FPS: \", 1.0 / (time.time() - start_time))\n",
    "        image = draw_boxes(image, boxes, scores, classes)\n",
    "\n",
    "        result.write(image)\n",
    "\n",
    "        print(\".\", end = '')\n",
    "    except:\n",
    "        print(\"ok\")\n",
    "        break\n",
    "    \n",
    "result.release() \n",
    "cv2.destroyAllWindows()\n",
    "n2cube.dpuDestroyTask(task)\n",
    "n2cube.dpuDestroyKernel(kernel)\n",
    "print(\"ok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
